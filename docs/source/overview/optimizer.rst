优化器
======

梯度下降优化器
::::::::::::::

随机梯度下降
>>>>>>>>>>>>

在随机梯度下降中，一次提供一个训练样本用于更新权重和偏置，从而使损失函数的梯度减小，然后再转向下一个训练样本。整个过程重复了若干个循环。由于
每次更新一次，所以它比 Vanilla 快，但由于频繁更新，所以损失函数值的方差会比较大。

Vanilla 梯度下降
>>>>>>>>>>>>>>>>

在 Vanilla 梯度下降（也称作批梯度下降）中，在每个循环中计算整个训练集的损失函数的梯度。该方法可能很慢并且难以处理非常大的数据集。该方法能保
证收敛到凸损失函数的全局最小值，但对于非凸损失函数可能会稳定在局部极小值处。

小批量梯度下降
>>>>>>>>>>>>>>

该方法结合了前两者的优点，利用一批训练样本来更新参数。


 Adam 优化器
::::::::::::


::

    https://arxiv.org/pdf/1609.04747.pdf：该文章提供了不同优化器算法的综述。
    https://www.tensorflow.org/api_guides/python/train#Optimizers：这是 TensorFlow.org 链接，详述了 TensorFlow 中优化器的使用方法。
    https://arxiv.org/pdf/1412.6980.pdf：关于 Adam 优化器的论文。



